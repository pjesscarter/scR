---
title: "recidivism"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{recidivism}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
This
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r, include=FALSE}
devtools::load_all()
library(parallel)
library(pbapply)
library(caret)
library(ggplot2)
library(plotly)
library(dplyr)
library(tidyr)
```
```{r setup}

library(scR)
```

# Simulating VC Dimension

The `simvcd()` function provides a simulation-based method for estimating the vcd dimension of any estimation algorithm. This function relies on two helper functions, `risk_bounds()` and `loss`, also included in the package, which provide estimates of the empirical risk for a given $n$ and define the sum of squares to be minimized, respectively. In the current version of the package, the estimation algorithm must follow the syntax of taking a `formula` object and rectangular `data.frame` as input and generating a binary factor variable as output. Future versions will support other common formats, as well as the provision of user-defined helper functions for other cases. Examples of common use cases are included in this vignette. 

In order to estimate the VC dimension for a given model, the user should provide the model (along with a `list` of any packages that need to be loaded), along with its dimension (i.e. the number of predictor variables) and simulation parameters $m,k,N$, which govern the number of simulations at each design point, the number of design points, and the maximum sample size, respectively. Note that consistency requires that all three of these parameters approach infinity, so that users must choose between computation time and the accuracy of the approximation. Below is an example of estimating the VC dimension for a small (2-variable) random forest using the default parameters. 

```{r,warning=FALSE}
library(randomForest)
simvcd(model=randomForest,dim=2,m=100,k=100,maxn=500,packages= list("randomForest"))
##randomforest == 994
```

# Replication example: Predicting Recidivism

In "The accuracy, fairness, and limits of predicting recidivism", Dressel and Farid (2018) use logistic regression to predict recidivism using an abridged list of 7 features. The replication data from their paper is included with the `scR` package. Here, we demonstrate how our method can be applied to this case in order to produce an estimate of the required sample size necessary to achieve targeted levels of accuracy.

First, we show how the `simvcd` function can be used to accurately approximate the true VCD of the linear discriminant model applied in the original paper, which in this case is known to be $8$ (one more than the number of features). In this case, we must write a custom function and prediction method that takes the output of the base `glm` from R and turns it into the expected format.

```{r}
mylogit <- function(formula, data){
  m <- structure(
    glm(formula=formula,data=data,family=binomial(link="logit")),
    class=c("svrclass","glm")  #IMPORTANT - must use the class svrclass to work correctly
  )
  return(m)
}
mypred <- function(m,newdata){
  out <- predict.glm(m,newdata,type="response")
  out <- factor(ifelse(out>0.5,1,0),levels=c("0","1")) #Important - must specify levels to account for possibility of all observations being classified into the same class in smaller samples
  return(out)
}
vcd <- simvcd(model=mylogit,dim=7,m=100,k=100,maxn=500,predictfn = mypred) #Takes about 20 minutes to run on mid-range test machine (Intel i5-11600K CPU)
```
This then allows us to estimate the minimum sample size $n$ needed to guarantee our chosen accuracy, given an assumed misclassification rate of $0.5$:

```{r}
scb(vcd,epsilon=0.05,delta=0.05,eta=0.05)
```
Note that we can also not specify the VCD and the function will run the simulation automatically, as long as it is provided with the appropriate
```{r,eval=FALSE}
scb(epsilon=0.05,delta=0.05,eta=0.05,theor=F,model=mylogit,dim=7,m=100,k=100,maxn=500,predictfn = mypred)
```
In this case, we know the true VCD dimension (8), so we can also assess the magnitude of the estimation error's impact on the required sample size:

```{r}
abs(scb(vcd,epsilon=0.05,delta=0.05,eta=0.05)-scb(8,epsilon=0.05,delta=0.05,eta=0.05))
```
## Estimating Accuracy Directly

The package also provides utility for users to directly estimate accuracy given assumptions on the data through the `estimate_accuracy` function. Since we already have data collected for this application, we can simply run the function directly on the original dataset:

```{r,warning=FALSE}
br <- scR::br
results <- estimate_accuracy(two_year_recid ~ race + sex + age + juv_fel_count + juv_misd_count + priors_count + charge_degree..misd.fel.,mylogit,br,predictfn = mypred)
```

The `plot_accuracy` function also provides a useful helper to quickly display the results of the simulation graphically using either the `ggplot2` or `plotly` libraries:

```{r}
fig <- plot_accuracy(results)
```

Or, using `plotly`:

```{r}
res2 <- getpac(results,delta = 0.05,epsilon=0.35) 
fig <- plot_accuracy(res2,plottype = "plotly")
fig
```

We can also avoid specifying the dataset, allowing the built-in `gendata` function to automatically benchmark synthetic data with the appropriate dimensionality:

```{r,warning=FALSE}
results <- estimate_accuracy(two_year_recid ~ race + sex + age + juv_fel_count + juv_misd_count + priors_count + charge_degree..misd.fel.,mylogit,maxn=nrow(br),dim=7,predictfn = mypred)
```
Unsurprisingly, the observed accuracy on the synthetic data is much more comparable to that expected based on theoretical bounds, since the target concept is ``learnable'' with the chosen algorithm by construction.

```{r}
res2 <- getpac(results,delta = 0.05,epsilon=0.05) 
fig <- plot_accuracy(res2,plottype = "plotly")
fig
```

If the researcher is interested in estimating the causal effect of the latent concept, the package also supports simultaneous estimation of the effect on statistical power.
```{r,warning=FALSE}
results <- estimate_accuracy(formula=two_year_recid ~ race + sex + age + juv_fel_count + juv_misd_count + priors_count + charge_degree..misd.fel.,
                             model=mylogit,
                             maxn=nrow(br),
                             dim=7,
                             predictfn = mypred,
                             power = T,
                             effect_size=0.5,
                             powersims=1000,
                             alpha=0.05)
```
```{r,warning=FALSE}
polyarchydata <- data %>% select(cont,part,polytrue) %>% unique()
results <- estimate_accuracy(formula=polytrue ~ cont + part,
                             model=mylogit,
                             data = polyarchydata,
                              nsample = 1000,
                             upperlimit = 500,
                             steps=100,
                             predictfn = mypred,
                             power = T,
                             effect_size=0.5,
                             powersims=1000,
                             alpha=0.05,
                             parallel = T,
                             coreoffset = 4)
res2 <- getpac(results,delta = 0.01,epsilon=0.05) 
plot_accuracy(res2,plottype="plotly")
```

# Works Cited

Dressel, J. and Farid, H., 2018. The accuracy, fairness, and limits of predicting recidivism. Science advances, 4(1), p.eaao5580.